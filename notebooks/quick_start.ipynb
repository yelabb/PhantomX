{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../python')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from phantomx.tokenizer import SpikeTokenizer\n",
    "from phantomx.vqvae import VQVAE, VQVAETrainer\n",
    "from phantomx.inference import LabramDecoder\n",
    "from phantomx.data import MCMazeDataLoader\n",
    "\n",
    "print(\"PhantomX loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e6f5e",
   "metadata": {},
   "source": [
    "## 1. POYO Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = SpikeTokenizer(\n",
    "    n_channels=142,\n",
    "    quantization_levels=16,\n",
    "    use_population_norm=True,\n",
    "    dropout_invariant=True\n",
    ")\n",
    "\n",
    "# Generate example spike data\n",
    "spike_counts = np.random.poisson(2.0, size=142)\n",
    "print(f\"Input: {spike_counts.shape} spike counts\")\n",
    "\n",
    "# Fit tokenizer (in practice, fit on training data)\n",
    "train_spikes = np.random.poisson(2.0, size=(1000, 142))\n",
    "tokenizer.fit(train_spikes)\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.tokenize(spike_counts)\n",
    "print(f\"Output: {tokens.shape} discrete tokens\")\n",
    "print(f\"Token values: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a2b64",
   "metadata": {},
   "source": [
    "## 2. VQ-VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = VQVAE(\n",
    "    n_tokens=16,\n",
    "    token_dim=256,\n",
    "    embedding_dim=64,\n",
    "    num_codes=256,\n",
    "    commitment_cost=0.25,\n",
    "    output_dim=2\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Codebook size: {model.num_codes}\")\n",
    "print(f\"Embedding dim: {model.embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8151dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MC_Maze data\n",
    "data_loader = MCMazeDataLoader(\n",
    "    data_path='../../PhantomLink/data/mc_maze.nwb',  # Adjust path\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "train_loader, val_loader, test_loader = data_loader.get_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa40dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model (short demo - use train_labram.py for full training)\n",
    "trainer = VQVAETrainer(model, learning_rate=3e-4)\n",
    "\n",
    "# Train for 5 epochs (demo)\n",
    "history = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=5,\n",
    "    save_dir='../models/demo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e186ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['reconstruction_loss'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE')\n",
    "axes[1].set_title('Reconstruction Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(history['perplexity'])\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Perplexity')\n",
    "axes[2].set_title('Codebook Utilization')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4760850b",
   "metadata": {},
   "source": [
    "## 3. Zero-Shot Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6aa435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder\n",
    "decoder = LabramDecoder(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    use_tta=False\n",
    ")\n",
    "\n",
    "# Single sample inference\n",
    "spike_counts = np.random.poisson(2.0, size=142)\n",
    "velocity = decoder.decode(spike_counts)\n",
    "print(f\"Predicted velocity: vx={velocity[0]:.3f}, vy={velocity[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6363d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference on test set\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    spikes = batch['spike_counts'].numpy()\n",
    "    targets = batch['kinematics'].numpy()\n",
    "    \n",
    "    preds = decoder.decode(spikes)\n",
    "    all_preds.append(preds)\n",
    "    all_targets.append(targets)\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "# Compute R²\n",
    "ss_res = np.sum((all_targets - all_preds) ** 2)\n",
    "ss_tot = np.sum((all_targets - np.mean(all_targets, axis=0)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"Zero-shot R² score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2571161",
   "metadata": {},
   "source": [
    "## 4. Test-Time Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adff5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable TTA\n",
    "decoder_tta = LabramDecoder(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    use_tta=True,\n",
    "    tta_strategy='entropy',\n",
    "    tta_lr=1e-4\n",
    ")\n",
    "\n",
    "# Test with drifted data\n",
    "# (In practice, use PhantomLink's NoiseInjectionMiddleware)\n",
    "drift = np.linspace(0, 1, 100)\n",
    "tta_preds = []\n",
    "\n",
    "for i in range(100):\n",
    "    spikes = np.random.poisson(2.0 + drift[i], size=142)\n",
    "    velocity = decoder_tta.decode(spikes)\n",
    "    tta_preds.append(velocity)\n",
    "\n",
    "tta_preds = np.array(tta_preds)\n",
    "\n",
    "# Get TTA statistics\n",
    "stats = decoder_tta.get_statistics()\n",
    "print(f\"Samples adapted: {stats['n_samples_adapted']}\")\n",
    "print(f\"Mean entropy: {stats['mean_entropy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3c0ad",
   "metadata": {},
   "source": [
    "## 5. Codebook Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2304fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get codebook embeddings\n",
    "codebook = model.get_codebook_embeddings().cpu().numpy()\n",
    "print(f\"Codebook shape: {codebook.shape}\")\n",
    "\n",
    "# PCA visualization\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "codebook_2d = pca.fit_transform(codebook)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(codebook_2d[:, 0], codebook_2d[:, 1], c=range(len(codebook)), cmap='viridis', s=50)\n",
    "plt.colorbar(label='Code Index')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Codebook Embedding Space (PCA)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b9c59b",
   "metadata": {},
   "source": [
    "## 6. Electrode Dropout Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robustness to electrode dropout\n",
    "dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "r2_scores = []\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    preds = []\n",
    "    targets = []\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        spikes = batch['spike_counts'].numpy()\n",
    "        target = batch['kinematics'].numpy()\n",
    "        \n",
    "        # Simulate electrode dropout\n",
    "        n_channels = spikes.shape[1]\n",
    "        dropout_mask = np.random.rand(n_channels) > dropout_rate\n",
    "        spikes_dropout = spikes * dropout_mask\n",
    "        \n",
    "        pred = decoder.decode(spikes_dropout)\n",
    "        preds.append(pred)\n",
    "        targets.append(target)\n",
    "    \n",
    "    preds = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "    \n",
    "    ss_res = np.sum((targets - preds) ** 2)\n",
    "    ss_tot = np.sum((targets - np.mean(targets, axis=0)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(dropout_rates, r2_scores, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Electrode Dropout Rate')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Decoder Robustness to Electrode Dropout')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.6, color='r', linestyle='--', label='Target (50% dropout)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"R² at 50% dropout: {r2_scores[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
