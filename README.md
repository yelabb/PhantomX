# PhantomX

> âš ï¸ **Experimental / Learning Project** - Not for production use

> ğŸ““ **[RESEARCH_LOG.md](RESEARCH_LOG.md)** - Detailed experiment notes, results, and analysis


PhantomX â€” Neural Decoding as a Codec: Quantized Latent Representations for Robust BCI

<img width="900" alt="image" src="https://github.com/user-attachments/assets/92cc2a54-26c8-48ae-bebb-34bd9925c11d" />

## ğŸ¯ Results

ğŸ”¬ **[Exp 23: Statistical Validation COMPLETE](RESEARCH_LOG.md#experiment-23-statistical-validation)**

### Validated Results (5 seeds each)

| Model | RÂ² (mean Â± std) | 95% CI | Verdict |
|-------|-----------------|--------|--------|
| ğŸ¥‡ **LSTM (aug)** | **0.8015 Â± 0.007** | [0.793, 0.810] | âœ… Practical winner |
| ğŸ¥ˆ LSTM (no aug) | 0.7936 Â± 0.007 | [0.785, 0.802] | Solid baseline |
| ğŸ¥‰ Wide Transformer (aug) | 0.7906 Â± 0.034 | [0.749, 0.833] | âš ï¸ High variance |

**Statistical Verdict**: âš ï¸ **INCONCLUSIVE** (p = 0.44) â€” No significant difference between models!

**Practical Verdict**: ğŸ† **LSTM wins** â€” 5x more stable, 3.4x faster, equivalent performance

### Leaderboard

| Rank | Model | RÂ² | Notes |
|------|-------|-----|-------|
| ğŸ¥‡ | **LSTM + Augmentation** | **0.8015 Â± 0.007** | Stable, fast, practical winner |
| ğŸ¥ˆ | LSTM (no aug) | 0.7936 Â± 0.007 | Still excellent |
| ğŸ¥‰ | Wide Transformer (384, 6L) | 0.7906 Â± 0.034 | Statistically equivalent, but unstable |
| 4 | [Distilled RVQ (Exp 19)](RESEARCH_LOG.md#experiment-19-distilled-rvq-combining-best-of-exp-12--exp-18) | 0.784 | Best discrete VQ |
| 5 | [RVQ-4 (Exp 12)](RESEARCH_LOG.md#experiment-12-residual-vector-quantization-rvq) | 0.776 | Discrete VQ |

## Key Findings

1. **Temporal context is essential**: Single timestep RÂ² â‰ˆ 0.10, with 250ms history RÂ² â‰ˆ 0.78
2. **POYO trade-off**: Full permutation invariance â†’ RÂ² â‰ˆ 0 (destroys velocity info)
3. **Codebook collapse**: Standard VQ training uses only 3-8% of codes
4. **Progressive training is key**: Pre-train â†’ k-means init â†’ finetune prevents collapse
5. **Residual VQ breaks Voronoi ceiling**: Multi-stage quantization captures fine details
6. **RVQ-4 optimal**: 4 layers Ã— 128 codes, more layers = diminishing returns
7. **FSQ topology doesn't help**: Ordinal code structure underperforms discrete VQ (Exp 14)
8. **Distillation eliminates VQ tax**: Exp 18/19 proved 0% discretization loss with latent distillation
9. **Lag tuning (Î”=+1) hurts**: Predicting 25ms ahead decorrelates signal on MC_Maze
10. **Student can beat teacher**: Exp 19 student (0.783) exceeded teacher (0.780) â€” RVQ acts as regularizer
11. **Î²=0.5 is optimal for distillation**: Exp 20 sweep showed higher Î² degrades performance (U-shaped curve)
12. **ğŸ”´ Long context (2s) HURTS on MC_Maze**: Exp 21 showed slow pathway degrades RÂ² by 2.8% â€” no exploitable preparatory dynamics
13. **250ms is optimal window**: Longer windows add noise, not signal for this dataset
14. **ğŸ‰ Width > Depth for Transformers**: Exp 21b showed 384Ã—6L (0.806) beats 256Ã—8L (0.793) and 512Ã—10L (0.805)
15. **Too deep hurts**: 384Ã—8L was WORST (0.752) â€” overfitting from excessive depth
16. **Data augmentation is CRITICAL during training**: Exp 21b used augment=True in sweep â†’ 0.806. Exp 22 forgot augmentation â†’ only 0.750
17. **Reproducibility requires matching ALL training conditions**: Architecture alone is insufficient â€” same augmentation, dropout, lr needed
18. **ğŸ”´ Exp 22 FAILED**: Teacher regressed 7% (0.806â†’0.750) without augmentation â†’ Student only reached 0.741
19. **Excellent codebook utilization**: Exp 22 achieved 94.5% average usage (484/512 codes) â€” no collapse issue
20. **ğŸ§  Inductive bias matters more than capacity**: Exp 23 showed LSTM (0.8015) beats Transformer (0.7906) because LSTM's sequential smoothing bias matches MC_Maze's simple reaching dynamics. Extra capacity without matching bias = variance, not performance.
21. **ğŸ”´ Exp 23 REFUTED Transformer claim**: Multi-seed validation showed Transformer is 1.4% worse, 5x less stable, and 3.4x slower than LSTM

## What This Is

An experimental project exploring:
- VQ-VAE based neural codebooks
- POYO-style spike tokenization
- Causal Transformer encoders with Gumbel-Softmax VQ
- Test-time adaptation for signal drift
- Zero-shot velocity decoding from motor cortex data

## Main Documentation

ğŸ““ **[RESEARCH_LOG.md](RESEARCH_LOG.md)** - Detailed experiment notes, results, and analysis

## Project Structure

```
python/phantomx/
    model.py           # ProgressiveVQVAE (MLP-based)
    models_extended.py # CausalTransformerVQVAE, GumbelVQVAE (best performers)
    trainer.py         # ProgressiveTrainer (3-phase training)
    tta.py             # Test-Time Adaptation (TTAWrapper, OnlineTTA)
    tokenizer/         # Spike tokenization
    data/              # MC_Maze data loading
python/
    exp10_beat_lstm.py # Latest: CausalTransformer + Gumbel experiments
    compare_models.py  # Model comparisons
models/
    exp9_progressive_vqvae.pt   # Progressive VQ-VAE (RÂ²=0.71)
    comparison_results.json     # All experiment results
```

## Quick Start

```python
from phantomx.model import ProgressiveVQVAE
from phantomx.trainer import ProgressiveTrainer
from phantomx.data import MCMazeDataset

# Load data
dataset = MCMazeDataset("path/to/mc_maze.nwb")
train_loader, val_loader, test_loader = create_dataloaders(dataset)

# Create and train model
model = ProgressiveVQVAE(n_channels=142, window_size=10)
trainer = ProgressiveTrainer(model, train_loader, val_loader)
result = trainer.train()
print(f"Best RÂ²: {result['best_r2']:.4f}")

# Test-Time Adaptation for new sessions
from phantomx.tta import OnlineTTA
tta = OnlineTTA(model)
predictions = tta.predict(new_data)
```

## Setup

```bash
python -m venv venv
venv\Scripts\activate  # Windows
pip install -r requirements.txt
```

## Current Status

âœ… **Exp 23 Complete** â€” Statistical validation with 5 seeds per model

### Key Result: LSTM Wins (Practically)

| Model | RÂ² (mean Â± std) | Stability | Speed |
|-------|-----------------|-----------|-------|
| ğŸ† **LSTM + Aug** | **0.8015 Â± 0.007** | Rock solid | 66s |
| LSTM (no aug) | 0.7936 Â± 0.007 | Solid | 71s |
| Transformer | 0.7906 Â± 0.034 | Unstable | 224s |

- **Statistical verdict**: âš ï¸ Inconclusive (p = 0.44)
- **Practical verdict**: ğŸ† LSTM â€” 5x more stable, 3.4x faster

### Best Discrete Model: Distilled RVQ (Exp 19)

- **RÂ² = 0.784** â€” Best VQ-based model
- Only 2.2% below LSTM baseline
- Discrete codebook with excellent utilization

See [RESEARCH_LOG.md](RESEARCH_LOG.md) for full experiment details


## Dream
<img width="450" alt="image" src="https://github.com/user-attachments/assets/af41ec36-29ea-4560-93bc-007247c36227" />

## Acknowledgments

This project was developed with assistance from AI coding assistants and workflows:
- Claude Opus 4.5 (Anthropic)
- Claude Sonnet 4.5 (Anthropic)
- Gemini 3.0 Pro (Google)
- GPT 5.2 (OpenAI)

All code was tested, and validated by the author.

## Author

ğŸ“§ youssef@elabbassi.com

If you use this work in your research, please cite:

```bibtex
@software{phantomx,
  author = {Youssef El Abbassi}
  title = {PhantomX: Neural Decoding as a Codec},
  year = {2026},
  url = {https://github.com/yelabb/PhantomX}
}
```



