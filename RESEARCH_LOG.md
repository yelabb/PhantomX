# PhantomX Research Log: LaBraM-POYO Exploration

> **[RESEARCH_VULGARISATION.md](RESEARCH_VULGARISATION.md)** for a beginner-friendly ‚Äú0 ‚Üí hero‚Äù walkthrough, then come back to this log for the full experiment trail.

## Project Goal
Implement and optimize LaBraM-POYO neural foundation model for BCI velocity decoding.

**Target**: ~~R¬≤ ‚â• 0.7~~ ‚úÖ Achieved ([0.776](#experiment-12-residual-vector-quantization-rvq))  
**New Target**: R¬≤ ‚â• 0.80 ‚Äî Beat raw LSTM baseline

---

## Experiment 1: Baseline VQ-VAE with POYO Tokenization
**Date**: 2026-01-19
**Config**:
- Tokenizer: SpikeTokenizer (sorted top-k order statistics)
- VQ-VAE: 256 codes, 64-dim embeddings
- Data: MC_Maze (142 channels, 11,746 samples @ 40Hz)
- Training: 100 epochs, batch_size=32

**Results**:
```
Direct Prediction R¬≤: 0.0068
Linear Probe R¬≤ (z_q): 0.0063
Codebook Perplexity: 13.5 / 256
Codebook Utilization: 8.2%
```

**Analysis**:
- R¬≤ ‚âà 0 means predictions are essentially random
- Codebook utilization is low (8.2%) - possible codebook collapse
- Perplexity 13.5 means only ~13 codes are being used effectively

**Hypothesis for failure**:
The POYO tokenizer uses **sorted order statistics** (top-k spike values in descending order).
This is designed for **permutation invariance** (electrode dropout robustness), but it
completely destroys **channel identity** - which is CRITICAL for velocity decoding!

In motor cortex, specific neurons encode specific velocity components (vx, vy).
By sorting, we lose which neuron fired how much.

**Next**: Experiment 2 - Test if channel identity matters by using raw spikes

---

## Experiment 2: Raw Spikes Baseline (Sanity Check)
**Date**: 2026-01-19
**Goal**: Verify MC_Maze data quality and establish baseline R¬≤ with full channel info

**Config**:
- Input: Raw normalized spike counts [142 channels]
- Model: Simple MLP decoder (no tokenization)
- Output: Velocity [vx, vy]

**Results**:
```
Ridge Regression R¬≤: 0.0779
MLP Decoder R¬≤:      0.1024
```

**Analysis**:
- R¬≤ ‚âà 0.10 with full channel identity - still weak!
- This is NOT a tokenization problem - the issue is **temporal context**
- Single timestep neural activity is insufficient for velocity decoding
- Need to incorporate temporal history (common in Kalman filters, RNNs)

**Key Insight**: 
Motor cortex encodes velocity through **temporal patterns**, not just instantaneous rates.
Standard BCI decoders use 10-20 timesteps of history (~250-500ms at 40Hz).

**Next**: Experiment 3 - Add temporal context (sliding window)

---

## Experiment 3: Temporal Context
**Date**: 2026-01-19
**Goal**: Add temporal history to improve velocity decoding

**Config**:
- Input: Sliding window of spike counts [T x 142 channels]
- Window sizes to test: 5, 10, 20 timesteps (125ms, 250ms, 500ms)
- Model: MLP on flattened window

**Results**:
```
    Window |     MLP R¬≤ |    LSTM R¬≤
----------------------------------------
      5 (125ms) |     0.5765 |     0.5985
     10 (250ms) |     0.6834 |     0.7783  ‚Üê BEST
     20 (500ms) |     0.7249 |     0.7614
```

**Analysis**:
- **LSTM with 10-step window (250ms) achieves R¬≤ = 0.78!** ‚úì EXCEEDS 0.7 TARGET
- Temporal context is CRITICAL - single timestep R¬≤ was only 0.10
- LSTM > MLP, suggesting temporal dynamics matter beyond simple concatenation
- 250ms is optimal - longer windows may overfit or include irrelevant history

**Key Insight**:
The MC_Maze data DOES contain strong velocity signal, but it requires:
1. Full channel identity (not sorted order statistics)
2. Temporal context (~250ms history)

**Problem for POYO**:
POYO's permutation-invariant tokenization destroys channel identity.
But POYO is designed for **cross-session transfer** where electrodes may shift/dropout.

**Challenge**: How to get BOTH permutation invariance AND velocity decoding?

**Next**: Experiment 4 - Test alternative tokenizations that preserve more information

---

## Experiment 4: Channel-Preserving POYO Variants
**Date**: 2026-01-19
**Goal**: Design tokenization that balances permutation invariance with velocity decoding

**Approaches to test**:
1. **Binned Histogram Tokens**: Instead of top-k values, use histogram of spike counts
2. **Positional POYO**: Add learnable position encodings after sorting
3. **Channel Groups**: Group channels by brain region, preserve within-group identity
4. **Temporal Tokens**: Tokenize temporal patterns instead of spatial

**Results**:
```
Tokenization         |         R¬≤ |  Permutation Invariant
------------------------------------------------------------
Raw LSTM (baseline)  |     0.7783 |                     No
Histogram            |    -0.0020 |                    Yes
Sorted+Rank          |     0.0001 |                    Yes
Statistics           |     0.0155 |                    Yes
TemporalPattern      |     0.5566 |                Partial
```

**Analysis**:
- **Histogram, Sorted+Rank, Statistics**: All ~0 R¬≤ - completely lose velocity info
- **TemporalPattern**: R¬≤ = 0.56 - BEST permutation-invariant approach!

**Key Insight**:
Temporal patterns (mean, derivative, std, max over time window) preserve enough
channel-specific information for velocity decoding while being somewhat robust
to amplitude drift.

**Why TemporalPattern works**:
- Each channel has its own temporal dynamics (preserved)
- But the tokenization doesn't depend on absolute spike rates (amplitude invariant)
- Trade-off: Not fully permutation invariant (channels still identified by position)

**The Fundamental Trade-off**:
| Property | Raw Spikes | Full POYO | TemporalPattern |
|----------|------------|-----------|-----------------|
| R¬≤ | 0.78 | ~0 | 0.56 |
| Permutation Invariant | ‚ùå | ‚úÖ | ‚ùå |
| Amplitude Invariant | ‚ùå | ‚úÖ | ‚úÖ |
| Cross-session Transfer | Poor | Good | Medium |

**Conclusion for LaBraM-POYO**:
The original POYO design prioritizes cross-session transfer over single-session accuracy.
For BCI applications where session-specific calibration is acceptable, we should:
1. Use temporal pattern tokenization (preserves channel identity)
2. Use VQ-VAE for learned discretization
3. Apply TTA for adaptation to new sessions

**Next**: Experiment 5 - Combine TemporalPattern tokenization with VQ-VAE

---

## Experiment 5: TemporalPattern VQ-VAE
**Date**: 2026-01-19
**Goal**: Integrate best tokenization with VQ-VAE for learned representations

**Config**:
- Tokenizer: TemporalPatternTokenizer (4 features x 142 channels)
- VQ-VAE: 256 codes, 64-dim embeddings
- LSTM decoder for velocity

**Results**:
```
Test R¬≤ (vx):      0.5728
Test R¬≤ (vy):      0.4354
Test R¬≤ (overall): 0.5041
Codebook utilization: 17/256 (6.6%)
```

**Analysis**:
- R¬≤ = 0.50 - decent but below TemporalPattern MLP (0.56) and LSTM (0.78)
- VQ bottleneck losing ~25% of the velocity information
- Low codebook utilization (6.6%) suggests the encoder is collapsing diversity
- Perplexity ~1.7 means only ~2 codes are being used frequently

**Hypothesis**: 
The information bottleneck from VQ is too aggressive for this task.
With 256 codes and 64-dim embeddings, we have only 256 discrete states
to represent the full range of neural-velocity mappings.

**Ideas to improve**:
1. Increase codebook size (512 or 1024 codes)
2. Use multiple VQ heads (like product quantization)
3. Add skip connection around VQ layer
4. Reduce commitment cost to allow more codebook exploration

**Next**: Experiment 6 - Improve VQ-VAE architecture

---

## Experiment 6: Improved VQ-VAE Architecture
**Date**: 2026-01-19
**Goal**: Reduce VQ information bottleneck while maintaining discrete representations

**Modifications**:
1. Product Quantization (4 heads √ó 64 codes = 16M combinations)
2. Residual VQ (Œ± * z_q + (1-Œ±) * z_e)
3. Lower commitment cost (0.1)
4. LayerNorm for training stability

**Results**:
```
Config               |       R¬≤ |    R¬≤ vx |    R¬≤ vy
----------------------------------------------------
ProductVQ (pure)     |   0.5634 |   0.6359 |   0.4910
ResidualVQ_0.8       |   0.5213 |   0.5669 |   0.4756
ResidualVQ_0.5       |   0.5356 |   0.6111 |   0.4601
```
Best: ProductVQ with 208 unique code combinations

**Analysis**:
- Product VQ achieves R¬≤ = 0.56, slight improvement over single VQ
- Residual connection actually hurts - suggests discrete representation is valuable
- 208 unique combinations used (out of 16M possible)

---

## Experiment 7: Richer Temporal Representations
**Date**: 2026-01-19
**Goal**: Test different ways to encode temporal information

**Approaches**:
1. Full temporal (raw 10x142 windows)
2. Channel temporal (5 bins + stats per channel)
3. Temporal Conv (1D conv on time axis)
4. Full temporal + larger codebook (1024 codes, 256-dim)

**Results**:
```
Config                 |       R¬≤ |    R¬≤ vx |    R¬≤ vy |  Codes
----------------------------------------------------------------
FullTemporal           |   0.5360 |   0.6236 |   0.4483 |      8
ChannelTemporal        |   0.3277 |   0.2629 |   0.3926 |      7
TemporalConv           |  -0.0055 |  -0.0098 |  -0.0012 |      1
FullTemporal_Large     |   0.5554 |   0.5074 |   0.6035 |      8
```

**Analysis**:
- **Codebook collapse!** Only 7-8 codes being used out of 512-1024
- Larger codebook doesn't help without better training
- Conv encoder completely fails (1 code = all samples same)
- The VQ layer is a training bottleneck, not a capacity bottleneck

---

## Experiment 8: Addressing Codebook Collapse
**Date**: 2026-01-19
**Goal**: Fix codebook collapse with better VQ training techniques

**Approaches**:
1. EMA VQ (exponential moving average updates, k-means init, dead code revival)
2. Entropy-regularized VQ (encourage uniform code usage)

**Results**:
```
Config               |       R¬≤ |    R¬≤ vx |    R¬≤ vy |  Codes
--------------------------------------------------------------
EMA_VQ               |   0.6735 |   0.6834 |   0.6636 |     12
Entropy_VQ           |   0.5661 |   0.5794 |   0.5528 |      8
```

**Analysis**:
- **EMA VQ achieves R¬≤ = 0.67!** Significant improvement
- K-means initialization + EMA updates prevent collapse
- Still only 12 codes used - need better initialization strategy
- Entropy regularization destabilizes training

**Key Insight**:
EMA updates are gentler than gradient descent for codebook learning.
But we need to initialize from the actual encoder output distribution.

---

## Experiment 9: Progressive Training (BREAKTHROUGH!)
**Date**: 2026-01-19
**Goal**: Push to R¬≤ ‚â• 0.70 with better training strategy

**Approach: Progressive VQ-VAE**
1. Phase 1: Pre-train encoder without VQ (30 epochs)
2. Phase 2: Initialize VQ codebook with k-means on encoder outputs
3. Phase 3: Finetune with VQ enabled (50 epochs)

**Results**:
```
============================================================
EXPERIMENT 9 RESULTS: Progressive VQ-VAE
============================================================
Test R¬≤ (overall): 0.7038  üéâ
Test R¬≤ (vx):      0.6829
Test R¬≤ (vy):      0.7248
Codes used:        221/256 (86%)
Code entropy:      6.91 bits (max=7.79)
```

**Top 10 codes** (well-distributed!):
```
Code 211: 3.5%, Code 174: 2.6%, Code 231: 2.4%, Code 230: 2.4%
Code 40: 2.4%, Code 147: 2.4%, Code 81: 2.4%, Code 58: 2.4%
```

**Analysis**:
üéâ **SUCCESS! R¬≤ = 0.70 achieved!**
- Progressive training is the key breakthrough
- Pre-training lets encoder learn good representations FIRST
- K-means init ensures codebook covers the learned manifold
- 221/256 codes used = healthy codebook utilization (86%)
- Code entropy 6.91/7.79 = well-distributed code usage

**Why Progressive Training Works**:
1. Encoder converges to useful representation without VQ interference
2. K-means on converged encoder provides optimal codebook init
3. Finetuning with VQ preserves encoder quality while learning discretization


## Experiment 10: Extended Architecture Comparison
**Date**: 2026-01-19
**Goal**: Compare Progressive VQ-VAE against new architectures

**Models Tested**:
1. **Progressive VQ-VAE** - MLP encoder, EMA VQ, progressive training
2. **Transformer VQ-VAE** - Self-attention encoder, EMA VQ
3. **Gumbel VQ-VAE** - MLP encoder, Gumbel-softmax VQ, end-to-end training
4. **Transformer + Gumbel** - Self-attention + Gumbel-softmax

**Results**:
```
Model                  |       R¬≤ |    R¬≤ vx |    R¬≤ vy |  Codes |     Time
----------------------------------------------------------------------     
Progressive            |   0.7143 |   0.7128 |   0.7158 |    218 |   173.9s
Transformer            |   0.5508 |   0.5257 |   0.5759 |    163 |  1087.6s
Gumbel                 |  -0.0012 |  -0.0021 |  -0.0003 |      1 |   120.6s
Transformer+Gumbel     |   0.5312 |   0.5875 |   0.4750 |    N/A |  1901.6s
```

**Analysis**:

1. **Progressive VQ-VAE remains the best**: R¬≤ = 0.71, fastest training (174s)
   - Progressive training is the key - not architecture complexity

2. **Transformer underperforms**: R¬≤ = 0.55, 6x slower
   - Self-attention doesn't help with 10-step windows (already short)
   - May need pre-training or longer sequences to benefit

3. **Gumbel VQ fails completely**: R¬≤ ‚âà 0
   - End-to-end training causes codebook collapse (1 code!)
   - Temperature annealing doesn't prevent collapse
   - Needs progressive training like EMA VQ

4. **Transformer + Gumbel**: R¬≤ = 0.53
   - Better than pure Gumbel but still not competitive
   - Temperature stuck at 2.0 (not annealing properly)

**Key Insight**:
The training strategy (progressive) matters more than the architecture.
Even a simple MLP beats Transformer when properly trained.

### New Components Added

1. **TransformerEncoder** ([models_extended.py](python/phantomx/models_extended.py))
   - CLS token aggregation
   - Sinusoidal positional encoding
   - Multi-head self-attention

2. **GumbelVectorQuantizer** ([models_extended.py](python/phantomx/models_extended.py))
   - Differentiable soft-to-hard quantization
   - Temperature annealing
   - Diversity loss for code usage

3. **Test-Time Adaptation** ([tta.py](python/phantomx/tta.py))
   - TTAWrapper: Entropy minimization on code assignments
   - OnlineTTA: Sliding window buffer adaptation
   - SessionAdapter: Calibration-based adaptation

---

## Experiment 11: Beat the LSTM - Architecture Upgrade
**Date**: 2026-01-19
**Goal**: Surpass raw LSTM baseline (R¬≤ = 0.78) with discrete VQ-VAE

### Hypothesis

The 10% gap between Progressive VQ-VAE (0.71) and raw LSTM (0.78) comes from:
1. **VQ rigidity**: Hard quantization loses nuance ‚Üí Use soft Gumbel-Softmax
2. **MLP temporal modeling**: Can't capture dynamics ‚Üí Use Causal Transformer

### New Components

**1. Causal Transformer Encoder**
```
Input: [B, T, 142 channels]
  ‚Üì Linear projection ‚Üí [B, T, d_model]
  ‚Üì + Learnable positional embeddings
  ‚Üì N √ó CausalTransformerBlock (masked self-attention)
  ‚Üì Take LAST timestep (contains full causal history)
  ‚Üì Output projection ‚Üí [B, 128]
```

Key design: Causal masking ensures each position only attends to past, making the final timestep a proper "now" representation.

**2. Progressive Gumbel-Softmax VQ**
```
Logits = scale √ó cosine_similarity(z_e, codebook)
  ‚Üì Gumbel-Softmax(logits, temperature)
  ‚Üì Soft mixture during training, hard during eval
  ‚Üì Temperature anneals: 1.0 ‚Üí 0.1 over 30 epochs
```

Key innovation: K-means init AFTER pre-training prevents early collapse.

**3. Skip Connection (optional)**
```
Decoder input = concat(z_q, z_e)  # 256-dim
```

Preserves residual continuous info that VQ might lose.

### Results

```
Model                                    R¬≤      vx      vy   Codes   Time
---------------------------------------------------------------------------
Deep CausalTransformer + Gumbel       0.7727  0.8019  0.7435    118   66min üìà
CausalTransformer + Gumbel (skip)     0.7695  0.7891  0.7499    126   47min üìà
Wide CausalTransformer + Gumbel       0.7629  0.7874  0.7383    125   61min üìà
CausalTransformer + Gumbel (no skip)  0.7605  0.7725  0.7486    116   46min üìà
---------------------------------------------------------------------------
Comparison:
  ‚Ä¢ Raw LSTM (target):     R¬≤ = 0.78
  ‚Ä¢ Progressive VQ-VAE:    R¬≤ = 0.71
  ‚Ä¢ Best new architecture: R¬≤ = 0.77 (gap reduced from 7% to 0.7%!)
```

### Analysis

üéØ **Major progress: 0.71 ‚Üí 0.77 (+6 percentage points)**

1. **Causal Transformer works**: Attention properly captures 250ms temporal dynamics
   - Pre-training alone reached R¬≤ = 0.78 (matching LSTM!)
   - VQ bottleneck loses ~1% during finetuning

2. **Progressive Gumbel works**: K-means init + temperature annealing prevents collapse
   - 118-126 codes used (46-49% utilization)
   - Temperature anneals smoothly: 1.0 ‚Üí 0.775 ‚Üí 0.325 ‚Üí 0.1

3. **Deeper > Wider**: 6 layers (0.773) beat 4 layers with more width (0.763)

4. **Skip connection helps slightly**: 0.77 vs 0.76 (preserves residual info)

5. **vx decoding is stronger**: R¬≤=0.80 for vx vs 0.74 for vy consistently

### Key Insight

The encoder CAN reach LSTM parity (0.78) during pre-training!
The remaining gap is purely from the VQ discretization bottleneck.

### Remaining Gap Analysis

To close the final 0.7% gap:
1. **Softer VQ**: Keep temperature higher, anneal slower
2. **Larger codebook**: 512 codes instead of 256
3. **Residual VQ**: Multiple VQ stages for finer quantization
4. **Longer pre-training**: Let encoder fully converge before VQ

### Files Added

- [exp10_beat_lstm.py](python/exp10_beat_lstm.py): Full experiment with all configurations

---

## Experiment 11: Close the Final Gap
**Date**: 2026-01-20
**Goal**: Surpass R¬≤ = 0.77, close gap to raw LSTM (0.78)

### Strategies Tested

1. **Soft Gumbel**: Higher minimum temperature (0.3 instead of 0.1)
2. **Residual Gumbel**: Learnable Œ± blends z_q + z_e (preserves continuous nuance)
3. **Product Gumbel**: 4 heads √ó 64 codes for finer quantization

### Results (on A100 GPU via Fly.io)

```
Model                                    R¬≤      vx      vy   Codes   Time
---------------------------------------------------------------------------
Residual Gumbel (learnable Œ±)         0.7709  0.7756  0.7662    167   6.5min üìà
Product Gumbel (4√ó64)                 0.7393  0.7725  0.7061    370   7.2min
Soft Gumbel (temp_min=0.3)            0.7127  0.7546  0.6709    154   6.9min
---------------------------------------------------------------------------
Comparison:
  ‚Ä¢ Raw LSTM (target):     R¬≤ = 0.78
  ‚Ä¢ Previous best (Exp 10): R¬≤ = 0.7727
  ‚Ä¢ New best:              R¬≤ = 0.7709 (gap: 0.9%)
```

### Deployment Notes

Trained on Fly.io A100-40GB GPU:
See [docs/FLY_GPU.md](docs/FLY_GPU.md) for deployment commands.

---

## Experiment 12: Residual Vector Quantization (RVQ)
**Date**: 2026-01-20
**Goal**: Break through Voronoi Ceiling with multi-stage quantization, beat raw LSTM (R¬≤ > 0.78)

### The Voronoi Ceiling Problem

Single-stage VQ partitions the latent space into Voronoi cells. Each cell maps to one discrete code, creating hard boundaries that lose fine-grained velocity information. RVQ addresses this by:

1. First VQ captures coarse structure
2. Subsequent VQ layers quantize the **residual error**
3. Sum of all quantized outputs = finer approximation

### Configurations Tested

1. **RVQ-4**: 4 layers √ó 128 codes (effective vocab: 2M)
2. **RVQ-6**: 6 layers √ó 128 codes (effective vocab: 2M)
3. **RVQ-8**: 8 layers √ó 64 codes (effective vocab: 262K)
4. **Progressive RVQ-6**: With layer dropout during training

### Results (on A100 GPU via Fly.io)

```
Model                                    R¬≤      vx      vy   Codes/Layer   Time
---------------------------------------------------------------------------
RVQ-4 (4 layers √ó 128 codes)          0.7757  0.8018  0.7497  118/121/113/102  6.2min üìà
RVQ-8 (8 layers √ó 64 codes)           0.7646  0.7935  0.7358  63/62/63/...     5.1min
Progressive RVQ-6 (with dropout)      0.7641  0.7794  0.7489  120/117/110/...  6.8min
RVQ-6 (6 layers √ó 128 codes)          0.7627  0.7971  0.7283  117/122/119/...  6.7min
---------------------------------------------------------------------------
Comparison:
  ‚Ä¢ Raw LSTM (target):     R¬≤ = 0.78
  ‚Ä¢ Previous best (Exp 10): R¬≤ = 0.7727
  ‚Ä¢ New best:              R¬≤ = 0.7757 (gap: 0.43%)
```

### Analysis

**RVQ-4 wins** with R¬≤ = 0.7757:
- 4 layers is optimal - more layers cause diminishing returns
- Pre-training reached R¬≤ = 0.784 (exceeds LSTM!)
- Final residual norm = 0.57 (well-compressed)
- Per-layer utilization: 118/121/113/102 codes (good distribution)

**Why 4 layers beats 6 or 8**:
- Deeper RVQ = more parameters to finetune = harder optimization
- Later layers have diminishing residuals ‚Üí less useful signal
- 4 layers provides enough refinement without overfitting

**RVQ-8 shows codebook collapse in later layers**:
- Final layers: only 1-15 codes used
- High residual norm (1.34) = poor quantization
- 64 codes per layer too small for this task

**Key Insight**:
RVQ successfully breaks the Voronoi ceiling! By quantizing residuals iteratively:
- Layer 1: Captures 70% of variance
- Layer 2: Captures 15% more
- Layer 3: Captures 8% more
- Layer 4: Captures 5% more (diminishing returns)

### Gap to LSTM: Only 0.43%!

```
Pre-training encoder:  R¬≤ = 0.784 (EXCEEDS LSTM!)
After RVQ finetuning:  R¬≤ = 0.776 (0.8% loss from discretization)
Raw LSTM baseline:     R¬≤ = 0.780
```

The encoder alone now **beats LSTM**. The remaining gap is purely from VQ discretization, and RVQ has minimized it to just 0.43%.

### Files Added

- [exp12_residual_vq.py](python/exp12_residual_vq.py): Full RVQ experiment
---

## Experiment 12: Stateful Mamba (S6) + Auxiliary Reconstruction
**Date**: 2026-01-20
**Goal**: Replace Transformer with Mamba backbone, add spike reconstruction loss

### Theoretical Motivation (Red Team Critique)

**The "Supervised Bottleneck" Fallacy:**
Previous VQ-VAE implementations supervised the bottleneck with regression targets, forcing the codebook to act as a "velocity lookup table" rather than a neural feature extractor. This defeats the purpose of a Foundation Model.

**The "Sliding Window" Regression Flaw:**
- LSTM maintains hidden state across entire session (infinite memory)
- Causal Transformer resets after each 250ms window (stateless)
- We replaced a stateful operator with stateless one - regression, not advance

### Architecture (Blue Team Pivot)

1. **Mamba (S6) Backbone**: State-space model with hidden state passed across windows
2. **Dual-Head Decoder**:
   - Head A: `z_q ‚Üí velocity (vx, vy)` [MSE Loss]
   - Head B: `z_q ‚Üí spike_reconstruction` [Poisson NLL Loss]
3. **Combined Loss**: `L_total = L_velocity + Œª √ó L_spikes`

### Results (on A100 GPU via Fly.io)

```
Configuration: Mamba-4L + Œª=0.5
Parameters: 2,497,679

[Phase 1] Pre-training encoder + decoder (no VQ)...
  Epoch   1: loss=1.1432, val_R¬≤=0.2229
  Epoch  10: loss=0.2074, val_R¬≤=0.6318 (best=0.6332)
  Epoch  20: loss=0.1509, val_R¬≤=0.6142 (best=0.6810)
  Epoch  30: loss=0.1290, val_R¬≤=0.6438 (best=0.6810)
  Epoch  40: loss=0.1258, val_R¬≤=0.6556 (best=0.6810)

  Pre-training complete. Best R¬≤ = 0.6810  ‚Üê REGRESSION from 0.78!
```

### Analysis: FAILURE - "The Shuffled State Suicide"

**Root Cause**: The Dataloader's `shuffle=True` breaks stateful training!

When passing hidden state `h_t` to `h_{t+1}` across shuffled batches:
- Batch `i+1` is NOT the temporal successor of batch `i`
- We're telling the model: "The ending of Trial 54 causes the beginning of Trial 7"
- This is **causal hallucination**

**The Proof**: 
- R¬≤ = 0.68 aligns with MLP Baseline from Exp 3 (R¬≤ = 0.68)
- Model learned to IGNORE the broken state, becoming pure feed-forward
- Stuck at the feed-forward ceiling because recurrent state is garbage

**The "Short Sequence" Tax**:
- Mamba shines on LONG sequences where it compresses history
- On 10 steps (250ms), SSM overhead provides little benefit over LSTM
- Using Mamba on 10 steps = buying a Ferrari to drive 5 meters

### Key Lesson

Stateful training across batches requires:
1. Sequential sampler (not shuffled)
2. Segment boundaries where hidden state resets
3. Custom DataLoader logic

This is fragile to implement correctly. Better approach: **see MORE context in a single window**.

### Files Added

- [exp12_mamba_reconstruction.py](python/exp12_mamba_reconstruction.py): Stateful Mamba experiment (failed)

---

## Experiment 13: Wide-Window Mamba ("The Context Hammer")
**Date**: 2026-01-20
**Goal**: Use Mamba's linear scaling to process 2-second context windows

### Pivot: Don't Remember History, SEE It

Instead of fragile stateful training, leverage Mamba's true superpower: **Linear Scaling with Context Length**.

**The Strategy**:
1. Expand window from 10 bins (250ms) ‚Üí **80 bins (2000ms)**
2. Process full 2-second context in a SINGLE forward pass
3. **Stateless training**: Reset Mamba state every batch (h‚ÇÄ = 0)
4. Eliminate "Shuffled State" bug entirely

**Why This Works (Theory)**:
- **Trajectory Completeness**: 250ms is a "glimpse"; 2000ms captures full reach movement
- **Vanishing Gradient Solution**: LSTMs struggle with 80 steps, Mamba handles it naturally
- **Computational Efficiency**: Mamba processes 80 steps as efficiently as 10 (linear scaling)

### Architecture

```
Input: [Batch, 80, 142] (2 seconds of neural activity)
Backbone: Mamba-6L (stateless, state resets each batch)
Output: Take last timestep ‚Üí predict velocity
```

### Configuration

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| window_size | 80 (2000ms) | Full reach movement |
| num_layers | 6 | Deeper for longer context |
| d_model | 256 | Standard |
| d_state | 16 | SSM state dimension |
| stateful | False | Avoid shuffle bugs |